@startuml
title Next.js Whisper API 處理流程（前端提取音訊）

actor User
participant "Next.js 前端" as FE
participant "FFmpeg.wasm (瀏覽器提取音訊)" as FFmpegWasm
participant "Next.js 後端" as BE
participant "OpenAI Whisper API" as Whisper
participant "OpenAI GPT API" as GPT

== 影片上傳 ==
User -> FE: 選擇並上傳影片

== 提取音訊 ==
FE -> FFmpegWasm: 1️⃣ 轉換影片為音訊 (.wav)
FFmpegWasm --> FE: 2️⃣ 返回 .wav 音訊 Blob

== Whisper API 轉錄 ==
FE -> BE: 3️⃣ POST /api/transcribe（附帶 .wav）
BE -> Whisper: 4️⃣ 傳送音訊 (.wav) 至 Whisper 進行語音轉錄（含時間戳）
Whisper --> BE: 5️⃣ 返回字幕 JSON

== 使用 GPT API 解析字幕 ==
BE -> GPT: 6️⃣ 傳送字幕內容給 GPT 分段並產生段落標題
GPT --> BE: 7️⃣ 返回結構化字幕 JSON（段落 + 標題）

== 返回結果 ==
BE -> FE: 8️⃣ 返回結構化 JSON (含段落 & 標題)
FE -> User: 顯示字幕於影片播放器中



@enduml