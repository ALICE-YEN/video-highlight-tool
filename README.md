# üìπ Video Highlight Tool

This project is a **Next.js-based video highlight editing tool**. It allows users to **upload a video**, automatically **generate transcripts**, and **select highlight segments** for a summarized playback.

**Live Demo**: [Your Deployed URL]  


## üõ†Ô∏è Tech Stack

| Category      | Technology           | Purpose                                      |
|--------------|----------------------|----------------------------------------------|
| **Frontend** | ![Next.js](https://img.shields.io/badge/Next.js-000?logo=next.js&logoColor=white)  | Supports SSR, SSG, CSR, and ISR |
|              | ![TailwindCSS](https://img.shields.io/badge/TailwindCSS-06B6D4?logo=tailwindcss&logoColor=white)  | Utility-first CSS framework |
|              | ![Framer Motion](https://img.shields.io/badge/Framer_Motion-0055FF?logo=framer&logoColor=white)  | Animation library for smooth UI interactions |
|              | ![React Context](https://img.shields.io/badge/React_Context-61DAFB?logo=react&logoColor=white)  | Global state management |
| **Video** | ![FFmpeg.wasm](https://img.shields.io/badge/FFmpeg.wasm-007808?logo=ffmpeg&logoColor=white) | This project intentionally avoids a dedicated backend. Built with **Next.js** and deployed on **Vercel‚Äôs Serverless platform**, it performs audio extraction directly in the **browser** using **ffmpeg.wasm** to convert video files into **Whisper-compatible .wav** format‚Äîbypassing single-request size limitations. |
| **Backend**  | ![Next.js API Routes](https://img.shields.io/badge/Next.js_API-000?logo=next.js&logoColor=white)  | Handles audio transcription, title generation, and highlight extraction via the `/api/transcribe` route using built-in Next.js API routes, powered by OpenAI Whisper and GPT. |
| **AI**       | ![OpenAI Whisper](https://img.shields.io/badge/Whisper_API-412991?logo=openai&logoColor=white)  | AI service for audio transcription |
|              | ![OpenAI GPT](https://img.shields.io/badge/OpenAI_GPT-412991?logo=openai&logoColor=white)  | AI service for title generation and selecting highlight sentences |


## üìå Features

### 1. Video Upload
- Users can upload a video file.
- Audio is extracted directly in the browser using **ffmpeg.wasm**, then sent to the backend for transcription via OpenAI.

### 2. Automatic AI Processing
- Uses OpenAI Whisper to generate **full video transcripts**.
- GPT API processes the transcript into **sections with titles**.
- The tool automatically suggests **highlighted sentences**.

### 3. Video Highlight Editing
- Two playback modes: **"Original"** (full video) and **"Highlighted"** (skips non-highlighted parts).
- Users can manually **select/unselect** highlight sentences.
- The timeline visually represents selected highlights.
- The video player **automatically updates preview content** when sentences are selected/unselected.

### 4. Interactive Transcript Editing
- Clicking on a timestamp **jumps to the corresponding moment** in the video.
- The transcript area **auto-scrolls** to keep the current sentence visible.
- Users can **edit subtitles and section titles** directly in the transcript.
- Edited content is **synchronized** with the video in real-time.

### 5. Subtitle Display
- The video player supports **real-time subtitles**.
- Users can **adjust subtitle font size** for better readability.

## üöÄ Installation & Setup

#### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/ALICE-YEN/video-highlight-tool.git
cd video-highlight-tool
```

#### 2Ô∏è‚É£ Install Dependencies
```bash
pnpm install
```

#### 3Ô∏è‚É£ Start the Development Server
```bash
pnpm dev
```


## üß© Next.js API Workflow
- **For a detailed overview of the Next.js API Workflow for Video & AI Transcription, refer to the Sequence Diagram (`NextJS_Whisper_Flow.wsd`).**
![image](https://github.com/user-attachments/assets/3c6e9051-580e-4336-b825-a7d3baed7a33)



## ‚öñÔ∏è Technical Choices  

### üéûÔ∏è FFmpeg Processing Options  

| Option | Evaluation | Pros | Cons | Constraints (Cost, Deployment, etc.) | Final Choice |
|--------|------------|------|------|------------------------------------|--------------|
| **Standalone Backend** | Run FFmpeg on a dedicated backend server (e.g., AWS Lambda, GCP, or a dedicated server) | ‚úÖ High performance for large videos <br> ‚úÖ Can handle multiple concurrent requests | ‚ùå Requires backend server maintenance <br> ‚ùå Higher cost for dedicated resources | Higher server costs, but scalable | ‚ùå Not chosen |
| **FFmpeg on Next.js Backend (FFmpeg on Node.js)** | Utilize Next.js API routes with `fluent-ffmpeg` to process video/audio | ‚úÖ Simplifies deployment using Vercel functions <br> ‚úÖ Lower cost than a dedicated backend <br> ‚úÖ Easier to integrate with the app | ‚ùå Limited performance for large files <br> ‚ùå Could hit memory limits on serverless environments | Works well within Vercel‚Äôs serverless limits for audio extraction | ‚úÖ **Chosen** |
| **FFmpeg in Browser (WASM-based FFmpeg)** | Process video/audio in the browser using `ffmpeg.wasm` | ‚úÖ No backend infrastructure needed <br> ‚úÖ Lower server costs <br> ‚úÖ Fully client-side execution | ‚ùå Poor performance for large files <br> ‚ùå High CPU usage on client devices | Limited by user‚Äôs device performance | ‚ùå Not chosen |

**Final Decision:**  
üìå **FFmpeg on Next.js Backend (FFmpeg on Node.js)** was chosen because it **strikes a balance between cost-efficiency and ease of deployment** while working well within **Vercel‚Äôs serverless function constraints** for **audio extraction**.



### üó£Ô∏è Whisper Speech-to-Text Options  

| Option | Evaluation | Pros | Cons | Constraints (Cost, Deployment, etc.) | Final Choice |
|--------|------------|------|------|------------------------------------|--------------|
| **@xenova/whisper in Browser** | Runs Whisper speech-to-text **entirely in the frontend** using WebAssembly (WASM) | ‚úÖ No backend cost <br> ‚úÖ Runs offline <br> ‚úÖ No API latency | ‚ùå High CPU/RAM usage <br> ‚ùå Poor performance on mobile devices <br> ‚ùå Large model downloads (~100MB+) | Requires powerful client devices to work efficiently | ‚ùå Not chosen |
| **@xenova/whisper on Backend (Node.js)** | Runs Whisper **on Next.js API routes** using Node.js | ‚úÖ No OpenAI API cost <br> ‚úÖ Runs within our own backend <br> ‚úÖ No request limits | ‚ùå Slower inference compared to OpenAI API <br> ‚ùå High resource usage for large files | Requires dedicated compute resources | ‚ùå Not chosen |
| **OpenAI Whisper API** | Uses OpenAI‚Äôs hosted Whisper API for speech-to-text conversion | ‚úÖ Fast inference times <br> ‚úÖ No resource constraints <br> ‚úÖ No model management required | ‚ùå API cost per request <br> ‚ùå Requires external API call (latency) | Pay-per-use pricing, but low effort to maintain | ‚úÖ **Chosen** |

**Final Decision:**  
üìå **OpenAI Whisper API** was chosen because it provides **fast, reliable speech-to-text processing** without the need to manage our own machine learning models or dedicate resources to **hosting** a Whisper instance.
